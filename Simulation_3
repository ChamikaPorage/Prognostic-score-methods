###############
# Simulation 3#
###############
# Suggested design applying exponential transformations and quadratic term to capture multiplicative effects

# Load required libraries
library(mlr3)
library(mlr3learners)
library(data.table)
library(xgboost)
library(ranger)
library(e1071)
library(caret)

gen.data.3 <- function(N, seed) {
  set.seed(seed)
  datat1 <- list()
  
  for (i in 1:1000) {

    # Generate baseline covariates
    X1 <- rnorm(N)
    X2 <- rnorm(N)
    X3 <- rnorm(N)
    X4 <- rnorm(N)
    X5 <- rbinom(N, 1, 0.5)
    X6 <- rnorm(N)
    X12 <- X1^2
    X22 <- X2^2
    X32 <- X3^2
    X42 <- X4^2
    X62 <- X6^2
    # Generate binary effect modifiers
    E1 <- rbinom(N, 1, 0.5)
    E2 <- rbinom(N, 1, 0.5)
    E3 <- rbinom(N, 1, 0.5)
    
    # Generate treatment variable
    p_t <-
      exp(0.15 * (sqrt(abs(X1+0.5)))^7 - 0.15 * (X2+0.35)^3 + 0.8 * (sin(X3 +0.4)) - 0.9 * sqrt(abs(X4)) + 0.5 * X5 + 0.5*X2*X3) / (1 + exp(0.15 * (sqrt(abs(X1+0.5)))^7 - 0.15 * (X2+0.35)^3 + 0.8 * (sin(X3 +0.4)) - 0.9 * sqrt(abs(X4)) + 0.5 * X5 + 0.5*X2*X3))
    Tr <- rbinom(N, 1, p_t)
    
    
    # Generate outcome variable
    mu <-
      -0.15 +  abs((X1^2+0.6))^1.5 /  exp(sqrt(abs(X4*X2)))  + (X4/6)^3  - 0.9 * sqrt(abs(X2))^3 + 0.8*sin (X3) + 0.5*X2*X3  -  exp(sqrt(abs(X6)))    - 2 * E3 + 5 *
      Tr + Tr * E1 + 4 * Tr * E2 - 4 * Tr * E3
    
    Y <- rnorm(N, mu, 1)
    
    # Creating data frame
    dat <- data.frame(Y = Y, Tr, X1, X2, X3, X4, X5, X6, X1*X2,X1*X3,X1*X4,X1*X6,X2*X3,X2*X4,X2*X6,X3*X4,X3*X6,X4*X6,X12, X22, X32, X42, X62, E1, E2, E3)
    
    # Subsetting data
    data0 <- subset(dat, Tr == 0)
    data1 <- subset(dat, Tr == 1)
    
    ###linear model
    
    # Linear model using data1
    pg1 <- lm(Y~ ., data= subset(data1, select = - Tr))
    p1<- predict(pg1,newdata = subset(dat, select = -c(Y, Tr)))           
    
    # Model using data0
    pg0 <- lm(Y~ ., data= subset(data0, select = - Tr))
    p0 <- predict(pg0, newdata = subset(dat, select = -c(Y, Tr)))
    
    ###Non-linear model
    
    # Create 5-fold cross-validation folds
    folds <- createFolds(dat$Y, k = 5, list = TRUE)
    
    # Initialize predictions
    p0ra <- p1ra <- p0gb <- p1gb <- p0svm <- p1svm <- rep(NA, N)
    
    for (k in 1:5) {
      # Training indices for nuisance parameter estimation
      train_idx <- unlist(folds[-k])  # Use all folds except k
      test_idx <- folds[[k]]         # Hold out fold k
      
      # Separate treatment groups in training data
      train_data <- dat[train_idx, ]
      test_data <- dat[test_idx, ]
      data0_train <- subset(train_data, Tr == 0)
      data1_train <- subset(train_data, Tr == 1)
      
      # Predict on the validation fold for causal effect estimation
      
      ### Random Forest (ranger)
      
      # Train random forest on untreated (Tr == 0)
      model_rf_0 <- ranger(Y ~ ., data = data0_train, num.trees = 300, mtry = 2, min.node.size = 5)
      p0ra[test_idx] <- predict(model_rf_0, data = test_data)$predictions
      
      # Train random forest on treated (Tr == 1)
      model_rf_1 <- ranger(Y ~ ., data = data1_train, num.trees = 300, mtry = 2, min.node.size = 5)
      p1ra[test_idx] <- predict(model_rf_1, data = test_data)$predictions
      
      ###XGBoost regression
      
      # Train XGBoost on untreated (Tr == 0)
      train_matrix_0 <- xgb.DMatrix(data = as.matrix(data0_train[, -c(1, 2)]), label = data0_train$Y)
      test_matrix <- xgb.DMatrix(data = as.matrix(test_data[, -c(1, 2)]))
      model_xgb_0 <- xgboost(data = train_matrix_0, max_depth = 6, eta = 0.05, nrounds = 50, objective = "reg:squarederror", min_child_weight = 5,
                             subsample = 0.8, colsample_bytree = 0.8, verbose = 0)
      p0gb[test_idx] <- predict(model_xgb_0, test_matrix)
      
      # Train XGBoost on treated (Tr == 1)
      train_matrix_1 <- xgb.DMatrix(data = as.matrix(data1_train[, -c(1, 2)]), label = data1_train$Y)
      test_matrix <- xgb.DMatrix(data = as.matrix(test_data[, -c(1, 2)]))
      model_xgb_1 <- xgboost(data = train_matrix_1, max_depth = 6, eta = 0.05, nrounds = 50, objective = "reg:squarederror", min_child_weight = 5,
                             subsample = 0.8, colsample_bytree = 0.8, verbose = 0)
      p1gb[test_idx] <- predict(model_xgb_1, test_matrix)
      
      ## SVM regression 
      # Train SVM on untreated (Tr == 0)
      model_svm_0 <- svm(Y ~ ., data = data0_train, type = "eps-regression", kernel = "radial", cost = 1, gamma = 0.1)
      p0svm[test_idx] <- predict(model_svm_0, newdata = test_data)
      
      # Train SVM on treated (Tr == 1)
      model_svm_1 <- svm(Y ~ ., data = data1_train, type = "eps-regression", kernel = "radial", cost = 1, gamma = 0.1)
      p1svm[test_idx] <- predict(model_svm_1, newdata = test_data)
      
    }
    
    # Creating squared and interaction terms
    p02 <- p0^2
    p12 <- p1^2
    p01 <- p0 * p1
    
    p1ra2 <- p1ra^2
    p0ra2 <- p0ra^2
    pra01 <- p1ra * p0ra
    
    p1gb2 <- p1gb^2
    p0gb2 <- p0gb^2
    pgb01 <- p1gb * p0gb
    
    p1svm2 <- p1svm^2
    p0svm2 <- p0svm^2
    psvm01 <- p1svm * p0svm
    
    datat1[[i]] <- as.data.frame(cbind(Y, Tr, p0, p1, p0ra, p1ra, p1gb, p0gb, p0svm, p1svm, p02, p12, p01, p1ra2, p0ra2, pra01, p1gb2, p0gb2, pgb01, p1svm2, p0svm2, psvm01 ))
    
  }
  return(datat1)
}


